
\section{System}\label{sec:System}
\todo[inline, color=yellow]{Laura}
The actual application can be divided into to type of VR rooms. First of all there is a learning room (compare section~\ref{sec:learningroom}), where the user can get in touch with the different interaction methods and afterwards different tasks will be presented to him in a VR supermarket scenario (compare section~\ref{sec:supermarket}). In the learning room the user will be supported in his learning process by a selfteaching system (compare section~\ref{sec:selfteaching}), which can get switched on and off, when he is in the supermarket. The user can make this setting among all other settings in a Menu  (compare section~\ref{sec:Menu}), which is controllable with the \textit{HTC Vive}-controller. 

\subsection{VR Labor}\label{sec:VRLabor}
\todo[inline, color=red]{Anna - first rough version}
In this section the different rooms of the VR Labor will be described. 

There are two different kinds of rooms. First there is a simple room to get familiar with the system. This room will be specified in the section \ref{sec:Learningroom}. The second kind of room is created to look like a small supermarket. Here the user has to solve different little tasks. This room will be specified in section \ref{sec:supermarket}.

\subsubsection{Learning room} \label{sec:Learningroom} %Bilder fehlen noch
The learning room is the first room where the user will experience the system. On one side the user will get familiar with the virtual experience and the HTC Vive system (HMD and controller). On the other side the user will learn the different interactions. 

The room is designed very clean and simple. In different sizes and distances simple objects, in this case cubes, are distributed randomly. The virtual room is slightly bigger than usable room in reality. So the user is forced to user interactions, which are designed for bigger distances, to move some objects. Due to different colors of the floor the user know how big the real room is approximately. Also the HTC Vive system offers a coloured wire which is shown if the user gets close to the border of the calibrated area.

%noch richtig doof
In order that the user already knows the different interactions as well as the menu settings, when he/she enters the supermarket, the menu and interactions are similar in both rooms. To get to know everything the user will be led to the entire learning room by a selfteaching system. This selfteaching will be described in section \ref{sec:selfteaching}. Also the labelling of the target area  and the reaction of this area to the target object will be established.

\subsubsection{Supermarket} \label{sec:supermarket} %Bilder fehlen noch
The second room is a small supermarket. Here are different sized objects in various distances. All objects are things that could be found in a real object, from fruits to milk.

Due to the fact that there are several objects, the user has to move other objects in some tasks before he/she reaches the labelled object. 

In the supermarket is the size of the real room also shown by the color of the floor. The supermarket is bigger than the real room. So interactions for far distance have to be used according to the tasks. Depending on the tasks different target areas can appear in the supermarket. In this situations different objects will be blend out. The different tasks will be described in section?. %\ref 

\subsection{Controller Menu} \label{sec:Menu}
\todo[inline, color=red]{Anna - first rough version} %Bilder fehlen noch
%Einstellen Interactions und weiterer einstellungen - Teaching/Tasks, Snaping, Reset, Start/Stop
The controller menu is the main menu of the system to change settings without leaving the virtual environment. Within this menu interaction methods can be changed and settings like snapping for the interactions can be enabled. In the learning room	the user is able to turn the selfteaching informations on and off. In the same way the user can decide if the tasks will be shown or not in the supermarket. In the supermarket is also the option available to start the task again. With the reset button the scene will be reloaded.

For the measurement, see section ?, the user has to start and stop every tasks. The start button will be the first menu button in every scene. Only if this button is pressed the user can interact with the environment and will be led to the next menu. When the user presses the stop button the switching area will appear and the corresponding box collider will be enabled. If the user reaches this area the next scene will be loaded.

%%Plugin, wie aufgebaut, Ã¤nderungen Unity, Script
The structure of the menu is built on the Plug-in "VRTK" (Virtual Reality Toolkit). %Referenz?
This is a free packet with different scripts and prefabs for interaction in the virtual reality. From this plug-in the radial menu is used for the controller menu. This creates menu buttons over the touchpad of the controller of the Vive. By scrolling over the touchpad the button can be selected. The selection will be shown by highlighting the button. If this button should be executed, the touchpad has to be pressed when the button is selected. The number of buttons per menu can be changed within the Unity editor. Each button can get a different icon. Through a link to a function of a script this function can be executed if the button is pressed. In this project there is a menu script in which all functions for the menu are collected. Due to this script different settings can be done. %so richtig schlecht...


\subsection{Interaction Methods}\label{sec:Interactions}
\todo[inline, color=yellow]{Laura}
Of course there were various different interaction methods required to make the \textit{Interaction Lab} suitable for the testing described and evaluated in section~\ref{sec:evaluation}. Also all interaction methods are implemented to realise the grabbing of virtual objects, there can be separated in the two categories, described in the next two paragraphs:

\paragraph{Close Range (CR) Interactions:} The CR can be interpreted as a synonym for the natural interaction radius of the person. Due to this definition it is excluded that those interactions can be used outside an area, which the person can not reach with his arm, or to be more precise: with the controller in his hand. In other words: the CR combines all interactions which can be used to pick up objects in the direct reach of the user.

\paragraph{Far Range (FR) Interactions:} Due to a limitation of the range of motion in VR applications, it is common to have grabbing interactions, which allow the users to grab objects which are normally seen as out of their reach~\cite{VRBook}. Interaction methods allowing such an acting are called FR interactions. \\

Whereas the CR interactions differ mainly in the accuracy of the selection of an object while grabbing it, the FR methods differ in their usability. All characteristics of the various interaction methods can be traced in their descriptions (compare sections~\ref{sec:TouchGrab} -~\ref{sec:RaycastHMD}).

For a better understanding it should be mentioned, that all interaction methods can be controlled with the \textit{HTC Vive}-controller. Even there are plenty of different possibilities to grab an object, all methods have in common, that the grabbing is caused by pressing the trigger on the \textit{HTC Vive}-controller. The releasing of the object is than triggered by letting it go. Whenever there is a divergent Usability necessary, it is described in the respective section (compare~\ref{sec:ExtendableRay}).

Due to an easier integration into the learning room (compare section~\ref{sec:learningroom}), as well as the actual supermarket scenes (compare section~\ref{sec:supermarket}) all methods using a ray (compare sections~\ref{sec:ExtendableRay},~\ref{sec:Raycast} and ~\ref{sec:WandGrab}) are summed up in one script called \textit{AllRaycastMethods.cs}. All other methods have their on script, where the grabbing and releasing is implemented. Also the \textit{Raycast Head Mounted Display}-method, described in section~\ref{sec:RaycastHMD}, is using a ray, it is not included into the script mentioned above. This is caused by remaining problems during the implementation of this method, which lead to an unfinished work. Further explanations on why this method is not available in the \textit{Interaction Lab} can be found in the according section.

The two interaction methods, described in sections~\ref{sec:TouchGrab} or rather~\ref{sec:ProximityGrab}, can be used with snapping or without it. This technique is used to reassign the position and orientation of a grabbed object in hand. By assuming that the middle of the ring of the \textit{HTC Vive}-controller is the new center of the grabbed object, the actual grab could appear more realistic to the user. 

In the application all objects, which can be grabbed are tagged as moveable.\\

In the following sections all available interaction methods of the \textit{Interaction Lab} are presented. To guarantee a better overview they are sorted by their interaction range. 

\subsubsection{Close Range: Touch Grab} \label{sec:TouchGrab}
\todo[inline, color=yellow]{Laura}
When this interaction method is selected, the user can make use of the \textit{HTC Vive}-controller to pick up objects directly by pulling the trigger. To release the object the trigger needs to be released as well. An object can be grabbed, whenever it is tagged as moveable and collides with the \textit{HTC Vive}-controller. This collision is detected by giving the object a collider, which fits its form best \cite{website:BoxCollider}\cite{website:SphereCollider} and applying a \textit{BoxCollider} to the controller (compare figure~\ref{fig:touchGrab}). In the script \textit{TouchGrab.cs} in which the interaction method is implemented there will be checked frequently, whether there is a overlap of the collider of the controller with the collider of a moveable object or not. Whenever they collide, the respective object is coloured green to show the user, that he could grab it by pulling the trigger. 

\begin{figure}[H] 
	\center 
	\includegraphics[width=12cm]{Images/TouchGrab.PNG}			
	\caption[Grabbing a virtual Object by using \textit{Touch Grab}.]{Grabbing a virtual Object by using \textit{Touch Grab}.}
	\label{fig:touchGrab}
\end{figure}

\subsubsection{Close Range: Proximity Grab} \label{sec:ProximityGrab}
\todo[inline, color=yellow]{Laura}
When it comes to the CR interactions the \textit{Proximity Grab} is by far the most inexact selection. Grabbing and releasing and object are realised by using the trigger of the \textit{HTC Vive}-controller.  \\
The functionality is provided by the script \textit{ProximityGrab.cs}. The basic idea is that the object can be grabbed, whenever the object triggers the \textit{BoxCollider} \cite{website:BoxCollider} placed at the end of the \textit{HTC Vive}-controller. A more detailed description can be found in section \ref{sec:TouchGrab} In contrast to the \textit{Touch Grab} described in section~\ref{sec:TouchGrab} this \textit{BoxCollider} is bigger than the actual size of the controller. To show the user which object collides with the controller and can therefore be grabbed the respective object is coloured green, as shown in figure~\ref{fig:proximityGrab}. The small gap between the actual grabbed object and the controller shows the difference between the interaction method shown in figure~\ref{fig:touchGrab}.

\begin{figure}[H] 
	\center 
	\includegraphics[width=12cm]{Images/ProximityGrab.PNG}			
	\caption[Grabbing a virtual Object by using \textit{Proximity Grab}.]{Grabbing a virtual Object by using \textit{Proximity Grab}.}
	\label{fig:proximityGrab}
\end{figure}


\subsubsection{Close Range: Wand Grab} \label{sec:WandGrab}
\todo[inline, color=yellow]{Laura}
In contrast to the interaction method described in \ref{sec:ProximityGrab} this method can be used to grab very tiny objects. Thereby it is not needed that the target object is very isolated from other objects. To give the user such an high grade of accuracy a stick is added to the controller like shown on figure~\ref{fig:wandGrab}. The user can grab an virtual object he touches with the \textit{HTC Vive}-controller by pulling the trigger. To place the object on the target area he simply release the trigger after he moved the object to its destination. \\
The implementation can be found in \textit{AllRaycastMethods.cs}. The wand consists of two elements: a ray \cite{website:Ray} and a cube. The cube is only for the visualisation and has a fixed size in all three dimensions. The collision detection, which is necessary for the actual grabbing, is done with the ray. That means, that an object can be grabbed, if the ray which has the same dimensions like the cube, touches this specific object. The cube will then turn from black to green to show the user, that there is an object, which can be grabbed.

\begin{figure}[H] 
	\center 
	\includegraphics[width=12cm]{Images/WandGrab.PNG}			
	\caption[Grabbing a virtual Object by using \textit{Wand Grab}.]{Grabbing a virtual Object by using \textit{Wand Grab}.}
	\label{fig:wandGrab}
\end{figure}

\subsubsection{Far Range: Raycast} \label{sec:Raycast}
\todo[inline, color=yellow]{Laura}
By using the \textit{Raycast} method, the user can grab virtual objects, which are further away, as well as objects in his CR. As shown in figure~\ref{fig:raycast} a ray is coming out of the \textit{HTC Vive}-controller pointing away from the user. At the end of the ray is a small sphere, which turns green, if it collides with an moveable object. Whenever the ray hits an objects, like for example the floor or a product in the supermarket (compare section~\ref{sec:supermarket}), the ray is shortened to the distance between the controller and the respective object. \\
The implementation can be found in the \textit{AllRaycastMethods.cs} script. As already explained in section \ref{sec:WandGrab} a cube and a ray are combined to reach the intended functionality. In contrast to the \textit{Wand Grab}, the ray and the visible cube have no fixed length and there is a sphere added to the end of the cube.

\begin{figure}[H] 
	\center 
	\includegraphics[width=12cm]{Images/Raycast.PNG}			
	\caption[Grabbing a virtual Object by using \textit{Raycast}.]{Grabbing a virtual Object by using \textit{Raycast}.}
	\label{fig:raycast}
\end{figure}


\subsubsection{Far Range: Extendable Ray} \label{sec:ExtendableRay}
\todo[inline, color=yellow]{Laura}
The actual ray is build as described in section \ref{sec:Raycast}. The ray \cite{website:Ray} is complemented by a cube with a sphere at the end, to make it visible for the user. In contrast to the normal \textit{Raycast} method, the length of the ray is set to a start value of 3 meters. The user can shorten and lengthen the ray by pressing the touchpad of the \textit{HTC Vive}-controller in the lower or rather upper area. This subtracts or adds a constant value to the length of the visible ray. What remains is the behaviour of the sphere, which turns green, whenever a moveable virtual object is brushed. The \textit{Extendable Ray} is one of the three interactions methods (compare sections~\ref{sec:WandGrab} and~\ref{sec:Raycast}), which are combined in the \textit{AllRaycastMethods.cs} script. This interaction method is shown on figure~\ref{sec:ExtendableRay}.

\begin{figure}[H] 
	\center 
	\includegraphics[width=12cm]{Images/ExtendableRay.PNG}			
	\caption[Grabbing a virtual Object by using \textit{Extendable Ray}.]{Grabbing a virtual Object by using \textit{Extendable Ray}.}
	\label{fig:extendableRay}
\end{figure} 


\subsubsection{Far Range: Raycast Head Mounted Display} \label{sec:RaycastHMD}
\todo[inline, color=yellow]{Laura}
It was planned to realise an interaction method, where there is a ray coming out of the HMD, which can be used similar to the method described in section~\ref{sec:Raycast}. Due to the low accuracy of this method, it did not become a part of the \textit{Interaction Lab}. There was a try to parent \cite{website:SetParent} the position of the HMD to the starting point of the ray. It turned out, that the parenting is not successful, when it comes to the position and rotation of the HMD. Even this method was effective for adding a ray to the \textit{HTC Vive}-controller, there is a irregular shift when you try to implement it with the HMD. To proof that an easy example (compare source code~\ref{lst:testHMD}) was observed. In this example a simple cube should be rendered at the front of the HMD. In reality this cube was rendered in a position, which can be seen as random. 

\lstinputlisting[title=\lstname, caption={Test on the parenting of \textit{HTC Vive}-HMD and an virtual object.}, label=lst:testHMD, language={[Sharp]C}, linerange=1, firstnumber=1]{SourceCode/HMDtest.cs} 

All effort on this method, can be found in the script \textit{RaycastingMethodeHMD.cs}.


\subsection{Self-Teaching} \label{sec:selfteaching}
\todo[inline, color=red]{Anna - first rough version, Probleme mit reinschreiben oder gesondertes kapitel?}
%Plugin Prefab fÃ¼r fahnen am controller und an Objekten, Step by step, alles erklÃ¤rt in Learning room, externes csv file mit Infotext und Button ID und hÃ¶he des FÃ¤hnchen, Probleme wenn schritte nicht genau eingehalten werden, nur bei auswahl der interactions und ausgewÃ¤hlten anderen Schritten feste Zahl fÃ¼r Counter. Counter wird aus verschiedenen Scripten bei bestimmten actions geÃ¤ndert. Task werden genauso angezeigt.

To introduce the user to the system there will be a teaching in the learning room. When the program will be started informations will be shown next to the controller. This informations led the user step by step to the system. He/She will get to know how the interaction methods can be changed and what settings can be made. This informations will just be shown in the learning room. The position of the information area changes depending on which button is important for the interaction. 

All informations and steps are saved in an external CSV-file. Due to an additional script (Name?) this file will be read line after line and be saved into different lists. In each line are three main informations. First there is the text which should be shown in this step. Second is the Button ID, which defines on which button the information should be fixed. Third and last is the hight of the information area. This hight changes depend on the amount of informations. 

In the selfteaching script is a counter implemented. This counter controls which line of the file will be displayed. This counter will be increased or set from different scripts depending on an action. This increasing is still a problem if the user does not follow the selfteaching exactly. For example, if the user grabs a object twice and the selfteaching plans to grab just once, the selfteaching will go on and the user misses information. 

The task are shown in the supermarket in the same way.

The information canvas is also a prefab of the "VRTK" plug-in. Here they are called controller tips. This creates a area next to the controller and a line from this area to the selected button of the controller. This area is parented to the controller and moves according to it. Different settings can be changed within the Unity Editor or by script.

The "VRTK" plug-in has also a prefab for object tips. This is also a canvas in which information can be display but this can be fixed on any object you like. With this canvas the target objects of the tasks are labelled in the supermarket.


\newpage